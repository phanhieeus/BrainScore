{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11920627,"sourceType":"datasetVersion","datasetId":7317087},{"sourceId":12027700,"sourceType":"datasetVersion","datasetId":7557251}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"86b707412de54ffa85f69c78d7480571":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_349d53a77e244ec088701bcfc99cc18b"],"layout":"IPY_MODEL_5015410ed96348b39e6a5b79cae07d25"}},"2ec4b4fb4e064afd9eaf2ba291120e11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b695b361a4b34b87a25835b5af9b0586","placeholder":"​","style":"IPY_MODEL_03d2769fde4b47e380d973cdf9e80a70","value":"<center> <img\nsrc=https://www.kaggle.com/static/images/site-logo.png\nalt='Kaggle'> <br> Create an API token from <a\nhref=\"https://www.kaggle.com/settings/account\" target=\"_blank\">your Kaggle\nsettings page</a> and paste it below along with your Kaggle username. <br> </center>"}},"7e8d8c570f7a4e33bf3351af4137cafb":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextView","continuous_update":true,"description":"Username:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_2474a29c59044abda97fa0fafa54fd80","placeholder":"​","style":"IPY_MODEL_d87a46fc65214f5aae742a428002b2b3","value":"ttn2807"}},"ac81cc5e40a84f66bdb5afb9daaa21a6":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_ed49021b9f8447a5b2af63c731cb600c","placeholder":"​","style":"IPY_MODEL_ccc1ea51b32a404583e408efebe68539","value":""}},"e3e5600c5d914fae93a2cb69462dd028":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_a5c85673c8f547988844ab2fa8e4cc25","style":"IPY_MODEL_db9b739f1ee0408b89e92a2b42ac4420","tooltip":""}},"05ee0761ff194b5d831ee382a1e1d0a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb64504d4e1649de933d7a88bf2f5509","placeholder":"​","style":"IPY_MODEL_ab129bcdaa2f47aaa96ad17e36cb0f14","value":"\n<b>Thank You</b></center>"}},"5015410ed96348b39e6a5b79cae07d25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"b695b361a4b34b87a25835b5af9b0586":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03d2769fde4b47e380d973cdf9e80a70":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2474a29c59044abda97fa0fafa54fd80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d87a46fc65214f5aae742a428002b2b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed49021b9f8447a5b2af63c731cb600c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccc1ea51b32a404583e408efebe68539":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5c85673c8f547988844ab2fa8e4cc25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db9b739f1ee0408b89e92a2b42ac4420":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"bb64504d4e1649de933d7a88bf2f5509":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab129bcdaa2f47aaa96ad17e36cb0f14":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3aa2c7bf6fe441991e444bd0ad284d4":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19a0915dab2e41b9bfeb4f4f00623eea","placeholder":"​","style":"IPY_MODEL_12abe82837de4bd9bb3bf080939b493a","value":"Connecting..."}},"19a0915dab2e41b9bfeb4f4f00623eea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12abe82837de4bd9bb3bf080939b493a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"349d53a77e244ec088701bcfc99cc18b":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d7bc82ebac4451f9fa155d528c2250a","placeholder":"​","style":"IPY_MODEL_262356e006ae43c7b733431fde17ef15","value":"Kaggle credentials successfully validated."}},"1d7bc82ebac4451f9fa155d528c2250a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"262356e006ae43c7b733431fde17ef15":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Library","metadata":{"id":"0TwAoYEuuWaR"}},{"cell_type":"code","source":"!pip install torchio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T16:13:26.293511Z","iopub.execute_input":"2025-06-01T16:13:26.293776Z","iopub.status.idle":"2025-06-01T16:14:42.734980Z","shell.execute_reply.started":"2025-06-01T16:13:26.293757Z","shell.execute_reply":"2025-06-01T16:14:42.734054Z"},"id":"uR4BTOtjuWaS","outputId":"ff46a852-f2c4-4bb9-c98f-500d3b68e23c"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport nibabel as nib\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nfrom torchvision.models.video import r3d_18\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\nimport math\n\nfrom tqdm import tqdm\n\nimport torchio as T\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-06-01T16:14:52.336191Z","iopub.execute_input":"2025-06-01T16:14:52.336544Z","iopub.status.idle":"2025-06-01T16:14:52.341422Z","shell.execute_reply.started":"2025-06-01T16:14:52.336527Z","shell.execute_reply":"2025-06-01T16:14:52.340682Z"},"trusted":true,"id":"TSOy1948uWaS"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def set_seed(seed=42):\n    np.random.seed(seed)\n    torch.manual_seed(seed)                   # PyTorch CPU\n    torch.cuda.manual_seed(seed)              # PyTorch GPU (1 GPU)\n    torch.cuda.manual_seed_all(seed)          # PyTorch GPU (all GPUs)\n    torch.backends.cudnn.deterministic = True # CUDNN deterministic\n    torch.backends.cudnn.benchmark = False    # Turn off speed autotune\n\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T16:14:52.343092Z","iopub.execute_input":"2025-06-01T16:14:52.343459Z","iopub.status.idle":"2025-06-01T16:14:52.367421Z","shell.execute_reply.started":"2025-06-01T16:14:52.343423Z","shell.execute_reply":"2025-06-01T16:14:52.366858Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load dataset","metadata":{"id":"kDuEoGA9uWaT"}},{"cell_type":"code","source":"class AlzheimerDataset(Dataset):\n    def __init__(self, \n                 image_folder,\n                 data_csv,\n                 transform=None):\n        \"\"\"\n        Args:\n            image_folder: Path to folder containing image_id subfolders\n            data_csv: Path to CSV file containing all data (demographics, cognitive scores, follow-up)\n            transform: Optional transforms for MRI images\n        \"\"\"\n        \n        self.image_folder = image_folder\n        self.transform = transform\n        \n        # Load CSV file\n        self.data_df = pd.read_csv(data_csv)\n        \n        # Process data\n        self.data_df = self._process_data()\n        \n        # Filter valid samples (has image file and all required data)\n        self.valid_samples = self._filter_valid_samples()\n        \n    def _process_data(self):\n        \"\"\"Process the data from CSV file\"\"\"\n        df = self.data_df.copy()\n        \n        # Convert gender to numeric\n        df['PTGENDER'] = df['PTGENDER'].astype(np.float32)\n                \n        return df\n    \n    def _filter_valid_samples(self):\n        \"\"\"Filter samples that have valid image files and complete data\"\"\"\n        valid_samples = []\n        \n        # Check if image folder exists\n        if not os.path.exists(self.image_folder):\n            return valid_samples\n        \n        for idx, row in self.data_df.iterrows():                \n            image_id = row['image_id']\n            \n            # Construct folder path with \"I\" prefix\n            folder_name = f\"I{image_id}\"\n            image_path = os.path.join(self.image_folder, folder_name)\n            \n            if not os.path.exists(image_path):\n                continue\n            \n            # Look for the specific file name: T1_biascorr_brain.nii.gz\n            nii_file_path = os.path.join(image_path, \"T1_biascorr_brain.nii.gz\")\n            \n            if not os.path.exists(nii_file_path):\n                # If specific file doesn't exist, look for any .nii or .nii.gz file\n                if os.path.exists(image_path):\n                    files_in_folder = os.listdir(image_path)\n                    nii_files = [f for f in files_in_folder \n                                if f.endswith('.nii') or f.endswith('.nii.gz')]\n                    if len(nii_files) == 0:\n                        continue\n                    nii_file_path = os.path.join(image_path, nii_files[0])\n            \n            # Check for required data completeness\n            required_cols = [\n                'PTGENDER', 'age', 'PTEDUCAT', 'time_lapsed', 'DIAGNOSIS_now',\n                'ADAS11_future', 'ADAS13_future', 'MMSCORE_future', 'CDGLOBAL_future'\n            ]\n            missing_cols = [col for col in required_cols if col not in row or pd.isna(row[col])]\n            \n            if missing_cols:\n                continue\n            \n            # Store file path and index\n            valid_samples.append({\n                'idx': idx,\n                'image_path': nii_file_path,\n                'image_id': image_id\n            })\n        \n        return valid_samples\n    \n    def _load_mri_image(self, image_path):\n        \"\"\"Load and preprocess MRI image\"\"\"\n        # Load nii file\n        img = nib.load(image_path)\n        data = img.get_fdata()\n        \n        # Convert to float32 and add channel dimension\n        data = data.astype(np.float32)\n        \n        # Normalize intensity to [0, 1]\n        data = (data - data.min()) / (data.max() - data.min() + 1e-8)\n        \n        # Convert to tensor: (D, H, W)\n        data_tensor = torch.from_numpy(data)\n        \n        # Repeat single channel to create 3 channels: (3, D, H, W)\n        data_tensor = data_tensor.unsqueeze(0).repeat(3, 1, 1, 1)\n        \n        # Apply transforms if provided\n        if self.transform:\n            data_tensor = self.transform(data_tensor)\n        \n        return data_tensor\n    \n    def __len__(self):\n        return len(self.valid_samples)\n    \n    def __getitem__(self, idx):\n        # Get sample info\n        sample_info = self.valid_samples[idx]\n        df_idx = sample_info['idx']\n        image_path = sample_info['image_path']\n        \n        # Get row data\n        row = self.data_df.iloc[df_idx]\n        \n        # Load MRI image\n        mri_image = self._load_mri_image(image_path)\n\n        # Get demographics\n        demographics = torch.tensor([\n            row['age'],\n            row['PTGENDER'],\n            row['PTEDUCAT'],\n            row['DIAGNOSIS_now'],\n            row['time_lapsed']\n        ], dtype=torch.float32)\n        \n        # Get cognitive scores\n        scores = {\n            'ADAS11_future': row['ADAS11_future'],\n            'ADAS13_future': row['ADAS13_future'],\n            'MMSCORE_future': row['MMSCORE_future'],\n            'CDGLOBAL_future': row['CDGLOBAL_future']\n        }\n        \n        # Convert to tensors\n        targets = {\n            'adas11_future': torch.tensor(scores['ADAS11_future'], dtype=torch.float32),\n            'adas13_future': torch.tensor(scores['ADAS13_future'], dtype=torch.float32),\n            'mmscore_future': torch.tensor(scores['MMSCORE_future'], dtype=torch.float32),\n            'cdglobal_future': torch.tensor(scores['CDGLOBAL_future'], dtype=torch.float32)\n        }\n        \n        return mri_image, demographics, targets\n\n# Custom transforms for 3D MRI data\nclass Resize3D:\n    def __init__(self, size):\n        self.size = size\n    \n    def __call__(self, tensor):\n        # tensor shape: (C, D, H, W)\n        # Resize using trilinear interpolation\n        tensor = torch.nn.functional.interpolate(\n            tensor.unsqueeze(0), \n            size=self.size, \n            mode='trilinear', \n            align_corners=False\n        ).squeeze(0)\n        return tensor\n\nclass RandomFlip3D:\n    def __init__(self, p=0.5):\n        self.p = p\n    \n    def __call__(self, tensor):\n        if torch.rand(1) < self.p:\n            # Random flip along each dimension\n            dims = []\n            if torch.rand(1) < 0.5:\n                dims.append(1)  # depth\n            if torch.rand(1) < 0.5:\n                dims.append(2)  # height\n            if torch.rand(1) < 0.5:\n                dims.append(3)  # width\n            \n            for dim in dims:\n                tensor = torch.flip(tensor, [dim])\n        return tensor\n\nclass Normalize3D:\n    def __init__(self, mean=0.5, std=0.5):\n        self.mean = mean\n        self.std = std\n    \n    def __call__(self, tensor):\n        return (tensor - self.mean) / self.std\n\n# Data loading utilities\ndef create_data_transforms(train=True, image_size=(64, 64, 64)):\n    \"\"\"Create transforms for MRI data\"\"\"\n    \n    if train:\n        transform = T.Compose([\n            T.Resize(image_size),\n            T.RandomFlip(axes=('LR',), p=0.5),\n            T.RandomAffine(scales=0.1, degrees=10, translation=5, p=0.75),\n            T.RandomNoise(std=0.01, p=0.5),\n            T.RandomMotion(p=0.3),\n            # T.ZNormalization()\n        ])\n    else:\n        transform = T.Compose([\n            T.Resize(image_size),\n            # T.ZNormalization()\n        ])\n    \n    return transform","metadata":{"execution":{"iopub.status.busy":"2025-06-01T16:14:52.368182Z","iopub.execute_input":"2025-06-01T16:14:52.368553Z","iopub.status.idle":"2025-06-01T16:14:52.386053Z","shell.execute_reply.started":"2025-06-01T16:14:52.368537Z","shell.execute_reply":"2025-06-01T16:14:52.385341Z"},"trusted":true,"id":"5Zbw5vR5uWaT"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_data_loaders(image_folder,\n                       data_csv,\n                       batch_size=8,\n                       val_split=0.1,\n                       test_split=0.1,\n                       image_size=(64, 64, 64)):\n    \"\"\"\n    Create train, validation, and test data loaders\n\n    Args:\n        image_folder: Path to MRI images folder\n        data_csv: Path to CSV file containing all data\n        batch_size: Batch size for training\n        val_split: Validation split ratio\n        test_split: Test split ratio\n        image_size: Target size for MRI images\n\n    Returns:\n        train_loader, val_loader, test_loader\n    \"\"\"\n\n    # Create full dataset\n    full_dataset = AlzheimerDataset(\n        image_folder=image_folder,\n        data_csv=data_csv,\n        transform=None  # Will be set later\n    )\n\n    # Calculate split sizes\n    dataset_size = len(full_dataset)\n    test_size = int(dataset_size * test_split)\n    val_size = int(dataset_size * val_split)\n    train_size = dataset_size - val_size - test_size\n\n    # Random split\n    train_indices, val_indices, test_indices = torch.utils.data.random_split(\n        range(dataset_size), [train_size, val_size, test_size]\n    )\n\n    # Create transforms\n    train_transform = create_data_transforms(train=True, image_size=image_size)\n    val_test_transform = create_data_transforms(train=False, image_size=image_size)\n\n    # Create subset datasets\n    train_dataset = torch.utils.data.Subset(full_dataset, train_indices.indices)\n    val_dataset = torch.utils.data.Subset(full_dataset, val_indices.indices)\n    test_dataset = torch.utils.data.Subset(full_dataset, test_indices.indices)\n\n    # Apply transforms\n    full_dataset.transform = train_transform\n\n    # Create data loaders\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=2,\n        pin_memory=True\n    )\n\n    val_loader = torch.utils.data.DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True\n    )\n\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=2,\n        pin_memory=True\n    )\n\n    print(f\"Data splits - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n\n    return train_loader, val_loader, test_loader\n\n# Example usage\ndef load_alzheimer_data(config):\n    \"\"\"\n    Load Alzheimer dataset with given configuration\n\n    config should contain:\n    - image_folder: path to images\n    - data_csv: path to data file\n    - batch_size: batch size\n    - image_size: tuple of target image dimensions\n    \"\"\"\n\n    train_loader, val_loader, test_loader = create_data_loaders(\n        image_folder=config['image_folder'],\n        data_csv=config['data_csv'],\n        batch_size=config.get('batch_size', 8),\n        image_size=config.get('image_size', (64, 64, 64)),\n        val_split=config.get('val_split', 0.1),\n        test_split=config.get('test_split', 0.1)\n    )\n\n    return train_loader, val_loader, test_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T16:14:52.386899Z","iopub.execute_input":"2025-06-01T16:14:52.387175Z","iopub.status.idle":"2025-06-01T16:14:52.401743Z","shell.execute_reply.started":"2025-06-01T16:14:52.387152Z","shell.execute_reply":"2025-06-01T16:14:52.401205Z"},"id":"vfu-WLstuWaU"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{"id":"Hc85rznRuWaV"}},{"cell_type":"code","source":"# # 3D CNN backbone for MRI images - keep original pretrained weights\n# cnn_backbone = r3d_18(pretrained=True)\n# # Remove final classification layer\n# cnn_backbone.fc = nn.Identity()\n# for param in cnn_backbone.parameters():\n#     print(param.requires_grad)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:48:30.164488Z","iopub.execute_input":"2025-06-01T15:48:30.164806Z","iopub.status.idle":"2025-06-01T15:48:30.178946Z","shell.execute_reply.started":"2025-06-01T15:48:30.164784Z","shell.execute_reply":"2025-06-01T15:48:30.178375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AlzheimerMultiTaskModel(nn.Module):\n    def __init__(self, pretrained=True, dropout_rate=0.3):\n        super(AlzheimerMultiTaskModel, self).__init__()\n        \n        # 3D CNN backbone for MRI images - keep original pretrained weights\n        self.cnn_backbone = r3d_18(pretrained=pretrained)\n        # Remove final classification layer\n        self.cnn_backbone.fc = nn.Identity()\n           \n        # Demographic network\n        self.demographic_net = nn.Sequential(\n            nn.Linear(5, 64),  # gender(1) + age(1) + education(1) + diagnosis(1) + time_future(1)\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(64, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate)\n        )\n        \n        # Feature fusion layer\n        cnn_features = 512        \n        demo_features = 128  # demographics future\n        fused_features = cnn_features + demo_features\n\n        self.fusion_layer = nn.Sequential(\n            nn.Linear(fused_features, 512),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(512, 256),\n            nn.ReLU(),\n            nn.Dropout(dropout_rate)\n        )\n\n        # Multi-task heads for follow-up scores\n        self.adas11_future = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n        \n        self.adas13_future = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(), \n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n        \n        self.mmscore_future = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(), \n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n        \n        self.cdglobal_future = nn.Sequential(\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n\n        # Initialize weights\n        self._initialize_weights()\n    \n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n    \n    def forward(self, mri_image, demographics):\n        # Extract features from MRI\n        mri_features = self.cnn_backbone(mri_image)\n        \n        # Process demographic data\n        demo_features = self.demographic_net(demographics)\n        \n        fused_features = torch.cat([mri_features, demo_features], dim=1)\n        shared_features = self.fusion_layer(fused_features)\n\n        # Follow-up scores predictions\n        adas11_future = self.adas11_future(shared_features)\n        adas13_future = self.adas13_future(shared_features)\n        mmscore_future = self.mmscore_future(shared_features)\n        cdglobal_future = self.cdglobal_future(shared_features)\n        \n        return {\n            'adas11_future': adas11_future.squeeze(-1),\n            'adas13_future': adas13_future.squeeze(-1),\n            'mmscore_future': mmscore_future.squeeze(-1),\n            'cdglobal_future': cdglobal_future.squeeze(-1)\n        }\n\nclass AlzheimerMultiTaskLoss(nn.Module):\n    def __init__(self, uncertainty_weighting=False):\n        super(AlzheimerMultiTaskLoss, self).__init__()\n        \n        # Default equal weights for all tasks\n        self.task_weights = {\n            'adas11_future': 3.0,\n            'adas13_future': 3.0,\n            'mmscore_future': 2.0,\n            'cdglobal_future': 1.0\n        }\n            \n        self.uncertainty_weighting = uncertainty_weighting\n        \n        # Learnable uncertainty parameters if using uncertainty weighting\n        if uncertainty_weighting:\n            self.log_vars = nn.Parameter(torch.zeros(8))  # 8 tasks now\n        \n        # Different loss functions for different score types\n        self.mse_loss = nn.MSELoss()\n        self.smooth_l1_loss = nn.SmoothL1Loss()\n    \n    def forward(self, predictions, targets):\n        losses = {}\n        total_loss = 0\n\n        # Follow-up scores losses with task-specific scaling\n        adas11_future_loss = self.mse_loss(predictions['adas11_future'], targets['adas11_future'])\n        adas13_future_loss = self.mse_loss(predictions['adas13_future'], targets['adas13_future'])\n        mmscore_future_loss = self.smooth_l1_loss(predictions['mmscore_future'], targets['mmscore_future'])\n        cdglobal_future_loss = self.smooth_l1_loss(predictions['cdglobal_future'], targets['cdglobal_future'])\n        \n        task_losses = [\n            adas11_future_loss, adas13_future_loss, mmscore_future_loss, cdglobal_future_loss\n        ]\n        task_names = [\n            'adas11_future', 'adas13_future', 'mmscore_future', 'cdglobal_future'\n        ]\n        \n        # Apply task weighting\n        if self.uncertainty_weighting:\n            # Uncertainty-based multi-task weighting\n            for i, (loss, name) in enumerate(zip(task_losses, task_names)):\n                precision = torch.exp(-self.log_vars[i])\n                weighted_loss = precision * loss + self.log_vars[i]\n                losses[f'{name}_loss'] = loss\n                total_loss += weighted_loss\n        else:\n            # Manual task weighting\n            for loss, name in zip(task_losses, task_names):\n                weighted_loss = self.task_weights[name] * loss\n                losses[f'{name}_loss'] = loss\n                total_loss += weighted_loss\n        \n        losses['total_loss'] = total_loss\n        return losses\n\n# Training utilities\nclass EarlyStopping:\n    def __init__(self, patience=10, min_delta=0.001):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.best_loss = float('inf')\n        \n    def __call__(self, val_loss):\n        if val_loss < self.best_loss - self.min_delta:\n            self.best_loss = val_loss\n            self.counter = 0\n            return False\n        else:\n            self.counter += 1\n            return self.counter >= self.patience\n\n# Example usage\ndef create_alzheimer_model(pretrained=True, dropout_rate=0.3, \n                          uncertainty_weighting=True):\n    \"\"\"\n    Create Alzheimer prediction model with custom loss\n    \n    Args:\n        pretrained: Use pretrained 3D ResNet backbone\n        dropout_rate: Dropout rate for regularization\n        uncertainty_weighting: Use learnable uncertainty weighting\n    \n    Returns:\n        model, loss_fn\n    \"\"\"\n    model = AlzheimerMultiTaskModel(\n        pretrained=pretrained,\n        dropout_rate=dropout_rate\n    )\n    \n    loss_fn = AlzheimerMultiTaskLoss(\n        uncertainty_weighting=uncertainty_weighting\n    )\n    \n    return model, loss_fn","metadata":{"execution":{"iopub.status.busy":"2025-06-01T16:14:52.402497Z","iopub.execute_input":"2025-06-01T16:14:52.402807Z","iopub.status.idle":"2025-06-01T16:14:52.420141Z","shell.execute_reply.started":"2025-06-01T16:14:52.402784Z","shell.execute_reply":"2025-06-01T16:14:52.419565Z"},"trusted":true,"id":"IIoWC5oHuWaV"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train function","metadata":{"id":"s_6jbCJ2uWaW"}},{"cell_type":"code","source":"def train_alzheimer_model(model, train_loader, val_loader, num_epochs=200, patience = 10, \n                         learning_rate=0.001, device='cuda'):\n    \"\"\"\n    Training function for Alzheimer prediction model\n    \"\"\"\n    model, loss_fn = create_alzheimer_model(\n            pretrained=True,          \n        uncertainty_weighting=True\n    )\n    model = model.to(device)\n    \n    # Optimizer with weight decay for regularization\n    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, \n                           weight_decay=1e-4)\n    \n    # Learning rate scheduler with warmup\n    num_warmup_steps = len(train_loader) * 5  # 5 epochs warmup\n    num_training_steps = len(train_loader) * num_epochs\n    \n    def lr_lambda(current_step):\n        if current_step < num_warmup_steps:\n            return float(current_step) / float(max(1, num_warmup_steps))\n        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n    \n    # Learning rate scheduler\n    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n    \n    # Early stopping\n    early_stopping = EarlyStopping(patience=patience, min_delta=0.001)\n    \n    best_val_loss = float('inf')\n    train_losses = []\n    val_losses = []\n    \n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        epoch_train_loss = 0\n        train_task_losses = {\n            'adas11_future': 0, 'adas13_future': 0, 'mmscore_future': 0, 'cdglobal_future': 0\n        }\n        \n        train_loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n        for mri_batch, demo_batch, targets_batch in train_loop:\n            mri_batch = mri_batch.to(device)\n            demo_batch = demo_batch.to(device)\n            targets_batch = {k: v.to(device) for k, v in targets_batch.items()}\n            \n            optimizer.zero_grad()\n            predictions = model(mri_batch, demo_batch)\n            losses = loss_fn(predictions, targets_batch)\n            \n            losses['total_loss'].backward()\n            \n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            epoch_train_loss += losses['total_loss'].item()\n            \n            # Accumulate task losses\n            for task in train_task_losses.keys():\n                train_task_losses[task] += losses[f'{task}_loss'].item()\n                \n            train_loop.set_postfix({'total_loss': losses['total_loss'].item()})\n        \n        # Calculate average task losses\n        for task in train_task_losses.keys():\n            train_task_losses[task] /= len(train_loader)\n        \n        # Validation phase\n        model.eval()\n        epoch_val_loss = 0\n        val_task_losses = {\n            'adas11_future': 0, 'adas13_future': 0, 'mmscore_future': 0, 'cdglobal_future': 0\n        }\n        val_predictions = {\n            'adas11_future': [], 'adas13_future': [], 'mmscore_future': [], 'cdglobal_future': []\n        }\n        val_targets = {\n            'adas11_future': [], 'adas13_future': [], 'mmscore_future': [], 'cdglobal_future': []\n        }\n        \n        scaler = {\n            'adas11_future': 70, 'adas13_future': 85, 'mmscore_future': 30, 'cdglobal_future': 3\n        }\n        \n        val_loop = tqdm(val_loader, desc=f\"Validation {epoch+1}/{num_epochs}\", unit=\"batch\")\n        with torch.no_grad():\n            for mri_batch, demo_batch, targets_batch in val_loop:\n                mri_batch = mri_batch.to(device)\n                demo_batch = demo_batch.to(device)\n                targets_batch = {k: v.to(device) for k, v in targets_batch.items()}\n                \n                predictions = model(mri_batch, demo_batch)\n                losses = loss_fn(predictions, targets_batch)\n                epoch_val_loss += losses['total_loss'].item()\n                \n                # Accumulate task losses\n                for task in val_task_losses.keys():\n                    val_task_losses[task] += losses[f'{task}_loss'].item()\n                \n                # Collect predictions and targets for metrics\n                for task in val_predictions.keys():\n                    val_predictions[task].extend(predictions[task].cpu().numpy())\n                    val_targets[task].extend(targets_batch[task].cpu().numpy())\n\n                val_loop.set_postfix({'total_loss': losses['total_loss'].item()})\n        # Calculate average task losses\n        for task in val_task_losses.keys():\n            val_task_losses[task] /= len(val_loader)\n        \n        # Calculate metrics\n        avg_train_loss = epoch_train_loss / len(train_loader)\n        avg_val_loss = epoch_val_loss / len(val_loader)\n        \n        train_losses.append(avg_train_loss)\n        val_losses.append(avg_val_loss)\n        \n        # Calculate R² and MAE for each task\n        metrics = {}\n        for task in val_predictions.keys():\n            tar = np.array(val_targets[task])* scaler[task]\n            pre = np.array(val_predictions[task]) * scaler[task]\n            r2 = r2_score(pre, tar)\n            mae = mean_absolute_error(pre, tar)\n            metrics[f'{task}_r2'] = r2\n            metrics[f'{task}_mae'] = mae\n        \n        print(f'Epoch {epoch+1}/{num_epochs}:')\n        print(f'  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n        print(f'  Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n        print('    🧠 ADAS11 FUTURE:')\n        print(f'    - Train Loss: {train_task_losses[\"adas11_future\"]:.4f}, Val Loss: {val_task_losses[\"adas11_future\"]:.4f}')\n        print(f'    - R²: {metrics[\"adas11_future_r2\"]:.4f}, MAE: {metrics[\"adas11_future_mae\"]:.4f}')\n        print('    🧠 ADAS13 FUTURE:')\n        print(f'    - Train Loss: {train_task_losses[\"adas13_future\"]:.4f}, Val Loss: {val_task_losses[\"adas13_future\"]:.4f}')\n        print(f'    - R²: {metrics[\"adas13_future_r2\"]:.4f}, MAE: {metrics[\"adas13_future_mae\"]:.4f}')\n        print('    🧠 MMSCORE FUTURE:')\n        print(f'    - Train Loss: {train_task_losses[\"mmscore_future\"]:.4f}, Val Loss: {val_task_losses[\"mmscore_future\"]:.4f}')\n        print(f'    - R²: {metrics[\"mmscore_future_r2\"]:.4f}, MAE: {metrics[\"mmscore_future_mae\"]:.4f}')\n        print('    🧠 CDGLOBAL FUTURE:')\n        print(f'    - Train Loss: {train_task_losses[\"cdglobal_future\"]:.4f}, Val Loss: {val_task_losses[\"cdglobal_future\"]:.4f}')\n        print(f'    - R²: {metrics[\"cdglobal_future_r2\"]:.4f}, MAE: {metrics[\"cdglobal_future_mae\"]:.4f}')\n        scheduler.step()\n        \n        # Save best model\n        if avg_val_loss < best_val_loss and avg_val_loss < avg_train_loss:\n            best_val_loss = avg_val_loss\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': best_val_loss,\n                'metrics': metrics\n            }, 'best_alzheimer_model.pth')\n            print(f'  ✅New best model saved at epoch {epoch+1} with best_val {best_val_loss:.4f}!!!')\n        # Early stopping\n        if early_stopping(avg_val_loss):\n            print(f\"‼️Early stopping at epoch {epoch+1}\")\n            break\n    \n    return train_losses, val_losses","metadata":{"execution":{"iopub.status.busy":"2025-06-01T16:16:24.308454Z","iopub.execute_input":"2025-06-01T16:16:24.309001Z","iopub.status.idle":"2025-06-01T16:16:24.325569Z","shell.execute_reply.started":"2025-06-01T16:16:24.308980Z","shell.execute_reply":"2025-06-01T16:16:24.324820Z"},"trusted":true,"id":"V3yWYSnuuWaW"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Main","metadata":{"id":"tgOfcDrduWaX"}},{"cell_type":"markdown","source":"### Configuration","metadata":{"id":"AUO_TG01uWaX"}},{"cell_type":"code","source":"# Configuration\nconfig = {\n    'image_folder': '/kaggle/input/fsl-brain-dataset/skull_stripped/skull_stripped/T1_biascorr_brain_data',\n    'data_csv': '/kaggle/input/csv-new-data/new_data.csv',\n    'batch_size': 4,\n    'image_size': (96, 96, 96),  # Adjust based on your memory\n    'val_split': 0.1,\n    'test_split':0.02\n}","metadata":{"execution":{"iopub.status.busy":"2025-06-01T16:17:54.533530Z","iopub.execute_input":"2025-06-01T16:17:54.533770Z","iopub.status.idle":"2025-06-01T16:17:54.537739Z","shell.execute_reply.started":"2025-06-01T16:17:54.533755Z","shell.execute_reply":"2025-06-01T16:17:54.536950Z"},"trusted":true,"id":"DONaDcAtuWaX"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2025-06-01T16:14:52.457578Z","iopub.execute_input":"2025-06-01T16:14:52.457838Z","iopub.status.idle":"2025-06-01T16:14:52.518614Z","shell.execute_reply.started":"2025-06-01T16:14:52.457824Z","shell.execute_reply":"2025-06-01T16:14:52.517852Z"},"trusted":true,"id":"4nJQJmX4uWaY","outputId":"1ccfb8bc-5688-4488-c554-f678c25a76e4"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Loading data...\")\ntrain_loader, val_loader, test_loader = load_alzheimer_data(config)\n# print(f\"Train: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}\")","metadata":{"execution":{"iopub.status.busy":"2025-06-01T16:17:57.949196Z","iopub.execute_input":"2025-06-01T16:17:57.949759Z","iopub.status.idle":"2025-06-01T16:17:58.419666Z","shell.execute_reply.started":"2025-06-01T16:17:57.949737Z","shell.execute_reply":"2025-06-01T16:17:58.418769Z"},"trusted":true,"id":"P_Ov5_ewuWaY","outputId":"1b31a2ae-7dae-4cd8-fa8f-6b7008e0b383"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, loss_fn = create_alzheimer_model(\n    pretrained=True,\n    uncertainty_weighting=True\n)\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T16:18:11.553823Z","iopub.execute_input":"2025-06-01T16:18:11.554604Z","iopub.status.idle":"2025-06-01T16:18:13.634082Z","shell.execute_reply.started":"2025-06-01T16:18:11.554579Z","shell.execute_reply":"2025-06-01T16:18:13.633521Z"},"id":"Lw4O7BwruWaY","outputId":"8bf0688e-48e0-416b-99ff-38c3ba7f079a"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training","metadata":{"id":"5-jSHYYAuWaY"}},{"cell_type":"code","source":"# Train model\nprint(\"Training model...\")\ntrain_losses, val_losses = train_alzheimer_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    num_epochs=200,\n    patience=20,\n    learning_rate=1e-4,\n    # learning_rate=0.001,\n    device=device\n)","metadata":{"execution":{"iopub.status.busy":"2025-06-01T16:18:19.993271Z","iopub.execute_input":"2025-06-01T16:18:19.993930Z","iopub.status.idle":"2025-06-01T16:18:34.296439Z","shell.execute_reply.started":"2025-06-01T16:18:19.993905Z","shell.execute_reply":"2025-06-01T16:18:34.295411Z"},"trusted":true,"id":"D9JFBV1DuWaY","outputId":"39ca408d-83fa-4a77-c43e-64499eb263c1"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Result","metadata":{"id":"OEdQnlQiuWaY"}},{"cell_type":"code","source":"# Update evaluation function\ndef evaluate_model(model, test_loader, device='cuda'):\n    \"\"\"Evaluate model on test set\"\"\"\n    model.eval()\n    predictions = {\n        'adas11_future': [], 'adas13_future': [], 'mmscore_future': [], 'cdglobal_future': []\n    }\n    targets = {\n        'adas11_future': [], 'adas13_future': [], 'mmscore_future': [], 'cdglobal_future': []\n    }\n    scaler = {\n        'adas11_future': 70, 'adas13_future': 85, 'mmscore_future': 30, 'cdglobal_future': 3\n    }\n    \n    test_loop = tqdm(test_loader, desc=\"Testing\", unit=\"batch\")\n    with torch.no_grad():\n        for mri_batch, demo_batch, targets_batch in test_loop:\n            mri_batch = mri_batch.to(device)\n            demo_batch = demo_batch.to(device)    \n            \n            preds = model(mri_batch, demo_batch)\n            \n            for task in predictions.keys():\n                predictions[task].extend(preds[task].cpu().numpy())\n                targets[task].extend(targets_batch[task].numpy())\n    \n    # Calculate final metrics\n    results = {}\n    for task in predictions.keys():\n        tar = np.array(targets[task]) * scaler[task]\n        pre = np.array(predictions[task]) * scaler[task]\n        r2 = r2_score(pre, tar)\n        mae = mean_absolute_error(pre, tar)\n        mse = mean_squared_error(pre, tar)\n        \n        results[task] = {\n            'r2_score': r2,\n            'mae': mae,\n            'mse': mse,\n            'predictions': pre,\n            'targets': tar\n        }\n    \n    return results ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:48:34.371982Z","iopub.execute_input":"2025-06-01T15:48:34.372352Z","iopub.status.idle":"2025-06-01T15:48:34.379203Z","shell.execute_reply.started":"2025-06-01T15:48:34.372327Z","shell.execute_reply":"2025-06-01T15:48:34.378498Z"},"id":"vAgeweC8uWaZ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load best model\nprint(\"Loading best model...\")\ncheckpoint = torch.load('/kaggle/input/brain_model/pytorch/default/4/best_alzheimer_model.pth', weights_only=False, map_location=torch.device('cpu'))\nmodel.load_state_dict(checkpoint['model_state_dict'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T15:48:34.379834Z","iopub.execute_input":"2025-06-01T15:48:34.380003Z","iopub.status.idle":"2025-06-01T15:48:36.999371Z","shell.execute_reply.started":"2025-06-01T15:48:34.379990Z","shell.execute_reply":"2025-06-01T15:48:36.998779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Load best model\n# print(\"Loading best model...\")\n# checkpoint = torch.load('best_alzheimer_model.pth', weights_only=False)\n# model.load_state_dict(checkpoint['model_state_dict'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-31T06:42:32.765508Z","iopub.execute_input":"2025-05-31T06:42:32.765779Z","iopub.status.idle":"2025-05-31T06:42:33.195518Z","shell.execute_reply.started":"2025-05-31T06:42:32.765762Z","shell.execute_reply":"2025-05-31T06:42:33.194966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_iter = iter(test_loader)\nall_results = []\nfor i in range(len(test_loader)):\n    print(f\"Test {i+1}:\")\n    model.eval()\n    mri_test, demographics_test, label_test = next(test_iter)\n    mri_test = mri_test.to(device)\n    demographics_test = demographics_test.to(device)\n    # Dự đoán\n    out = model(mri_test, demographics_test)\n\n    # Scale ngược lại các giá trị\n    adas11_future_pred = (out[\"adas11_future\"] * 70).cpu().detach().numpy().tolist()\n    adas11_future_actual = (label_test[\"adas11_future\"] * 70).cpu().detach().numpy().tolist()\n\n    adas13_future_pred = (out[\"adas13_future\"] * 85).cpu().detach().numpy().tolist()\n    adas13_future_actual = (label_test[\"adas13_future\"] * 85).cpu().detach().numpy().tolist()\n\n    mmscore_future_pred = (out[\"mmscore_future\"] * 30).cpu().detach().numpy().tolist()\n    mmscore_future_actual = (label_test[\"mmscore_future\"] * 30).cpu().detach().numpy().tolist()\n\n    cdglobal_future_pred = (out[\"cdglobal_future\"] * 3).cpu().detach().numpy().tolist()\n    cdglobal_future_actual = (label_test[\"cdglobal_future\"] * 3).cpu().detach().numpy().tolist()\n\n    print('🧠ADAS11 FUTURE:')\n    print(f'🔵Predicted: {adas11_future_pred}\\n🔴Actual:    {adas11_future_actual}')\n    print('🧠ADAS13 FUTURE:')\n    print(f'🔵Predicted: {adas13_future_pred}\\n🔴Actual:    {adas13_future_actual}')\n    print('🧠MMSCORE FUTURE:')\n    print(f'🔵Predicted: {mmscore_future_pred}\\n🔴Actual:    {mmscore_future_actual}')\n    print('🧠CDGLOBAL FUTURE:')\n    print(f'🔵Predicted: {cdglobal_future_pred}\\n🔴Actual:    {cdglobal_future_actual}')\n    print('=============================================================================================================')\nprint(\"Done!\")\n","metadata":{"execution":{"iopub.status.busy":"2025-06-01T15:49:10.939059Z","iopub.execute_input":"2025-06-01T15:49:10.939684Z","iopub.status.idle":"2025-06-01T15:49:39.245178Z","shell.execute_reply.started":"2025-06-01T15:49:10.939656Z","shell.execute_reply":"2025-06-01T15:49:39.244265Z"},"trusted":true,"id":"makWSLV5uWaZ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_fig(data, path):\n    import matplotlib.pyplot as plt\n    \n    test_iter = iter(data)\n    all_results = []\n    for i in range(len(data)):\n        model.eval()\n        mri_test, demographics_test, label_test = next(test_iter)\n        mri_test = mri_test.to(device)\n        demographics_test = demographics_test.to(device)\n        # Dự đoán\n        out = model(mri_test, demographics_test)\n    \n        # Scale ngược lại các giá trị\n        adas11_future_pred = (out[\"adas11_future\"] * 70).cpu().detach().numpy().tolist()\n        adas11_future_actual = (label_test[\"adas11_future\"] * 70).cpu().detach().numpy().tolist()\n    \n        adas13_future_pred = (out[\"adas13_future\"] * 85).cpu().detach().numpy().tolist()\n        adas13_future_actual = (label_test[\"adas13_future\"] * 85).cpu().detach().numpy().tolist()\n    \n        mmscore_future_pred = (out[\"mmscore_future\"] * 30).cpu().detach().numpy().tolist()\n        mmscore_future_actual = (label_test[\"mmscore_future\"] * 30).cpu().detach().numpy().tolist()\n    \n        cdglobal_future_pred = (out[\"cdglobal_future\"] * 3).cpu().detach().numpy().tolist()\n        cdglobal_future_actual = (label_test[\"cdglobal_future\"] * 3).cpu().detach().numpy().tolist()\n    \n        # Gộp lại thành từng sample 1 dòng\n        batch_size = len(cdglobal_future_pred)\n        for j in range(batch_size):\n            all_results.append({\n                \"Test\": i + 1,\n                \"ADAS11_FUTURE_pred\": round(adas11_future_pred[j], 3),\n                \"ADAS11_FUTURE_actual\": round(adas11_future_actual[j], 3),\n                \"ADAS13_FUTURE_pred\": round(adas13_future_pred[j], 3),\n                \"ADAS13_FUTURE_actual\": round(adas13_future_actual[j], 3),\n                \"MMSCORE_FUTURE_pred\": round(mmscore_future_pred[j], 3),\n                \"MMSCORE_FUTURE_actual\": round(mmscore_future_actual[j], 3),\n                \"CDGLOBAL_FUTURE_pred\": round(cdglobal_future_pred[j], 3),\n                \"CDGLOBAL_FUTURE_actual\": round(cdglobal_future_actual[j], 3)\n            })\n    df_results = pd.DataFrame(all_results)\n    plt.figure(figsize=(15, 15))\n    # Scatter plot ADAS13\n    plt.subplot(2, 2, 1)\n    plt.scatter(df_results['ADAS11_FUTURE_actual'], df_results['ADAS11_FUTURE_pred'], alpha=0.6, color='blue', label='ADAS11 FUTURE')\n    plt.plot([0, 70], [0, 70], 'r-')  # Đường chéo y = x để so sánh\n    plt.xlabel(\"Actual\")\n    plt.ylabel(\"Predict\")\n    plt.title(\"Score\")\n    plt.tight_layout()\n    plt.legend()\n    plt.subplot(2, 2, 2)\n    plt.scatter(df_results['ADAS13_FUTURE_actual'], df_results['ADAS13_FUTURE_pred'], alpha=0.6, color='blue', label='ADAS13 FUTURE')\n    plt.plot([0, 85], [0, 85], 'r-')  # Đường chéo y = x để so sánh\n    plt.xlabel(\"Actual\")\n    plt.ylabel(\"Predict\")\n    plt.title(\"Score\")\n    plt.tight_layout()\n    plt.legend()\n    plt.subplot(2, 2, 3)\n    plt.scatter(df_results['MMSCORE_FUTURE_actual'], df_results['MMSCORE_FUTURE_pred'], alpha=0.6, color='blue', label='MMSCORE FUTURE')\n    plt.plot([-2, 30], [-2, 30], 'r-')  # Đường chéo y = x để so sánh\n    plt.xlabel(\"Actual\")\n    plt.ylabel(\"Predict\")\n    plt.title(\"Score\")\n    plt.tight_layout()\n    plt.legend()\n    plt.subplot(2, 2, 4)\n    plt.scatter(df_results['CDGLOBAL_FUTURE_actual'], df_results['CDGLOBAL_FUTURE_pred'], alpha=0.6, color='blue', label='CDGLOBAL FUTURE')\n    plt.plot([0, 3], [0, 3], 'r-')  # Đường chéo y = x để so sánh\n    plt.xlabel(\"Actual\")\n    plt.ylabel(\"Predict\")\n    plt.title(\"Score\")\n    plt.tight_layout()\n    plt.savefig(path, dpi=300, bbox_inches='tight')\n    # plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate on test set\ndef print_test(data):\n    print(\"Evaluating...\")\n    test_results = evaluate_model(model, data, device)\n    \n    # Print results\n    print(\"\\nTest Results:\")\n    R2_metric = {\n        'adas11_future': 0, 'adas13_future': 0, 'mmscore_future': 0, 'cdglobal_future': 0\n    }\n    MAE_metric = {\n        'adas11_future': 0, 'adas13_future': 0, 'mmscore_future': 0, 'cdglobal_future': 0\n    }\n    MSE_metric = {\n        'adas11_future': 0, 'adas13_future': 0, 'mmscore_future': 0, 'cdglobal_future': 0\n    }\n    for task, metrics in test_results.items():\n        # print(f\"{task}:\")\n        # print(f\"  R² Score: {metrics['r2_score']:.4f}\")\n        # print(f\"  MAE: {metrics['mae']:.4f}\")\n        # print(f\"  MSE: {metrics['mse']:.4f}\")\n        R2_metric[task] = metrics['r2_score']\n        MAE_metric[task] = metrics['mae']\n        MSE_metric[task] = metrics['mse']\n    print(\"📊 Độ lệch trung bình (MAE):\")\n    print(\"🔵Tương lai\")\n    print(f\"  -ADAS11: {MAE_metric['adas11_future']:.4f} điểm\")\n    print(f\"  -ADAS13: {MAE_metric['adas13_future']:.4f} điểm\")\n    print(f\"  -MMSCORE: {MAE_metric['mmscore_future']:.4f} điểm\")\n    print(f\"  -CDGLOBAL: {MAE_metric['cdglobal_future']:.4f} điểm\")\n    print(\"📊 Sai số bình phương trung bình (MSE):\")\n    print(\"🔵Tương lai\")\n    print(f\"  -ADAS11: {math.sqrt(MSE_metric['adas11_future']):.4f} điểm²\")\n    print(f\"  -ADAS13: {math.sqrt(MSE_metric['adas13_future']):.4f} điểm²\")\n    print(f\"  -MMSCORE: {math.sqrt(MSE_metric['mmscore_future']):.4f} điểm²\")\n    print(f\"  -CDGLOBAL: {math.sqrt(MSE_metric['cdglobal_future']):.4f} điểm²\")\n    print(\"📊 Hệ số xác định (R² Score):\")\n    print(\"🔵Tương lai\")\n    print(f\"  -ADAS11: {R2_metric['adas11_future']:.4f} điểm\")\n    print(f\"  -ADAS13: {R2_metric['adas13_future']:.4f} điểm\")\n    print(f\"  -MMSCORE: {R2_metric['mmscore_future']:.4f} điểm\")\n    print(f\"  -CDGLOBAL: {R2_metric['cdglobal_future']:.4f} điểm\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Train**","metadata":{}},{"cell_type":"code","source":"print_test(train_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_fig(train_loader, 'train_plot.png')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Val**","metadata":{}},{"cell_type":"code","source":"print_test(val_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_fig(val_loader, 'val_plot.png')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Test**","metadata":{}},{"cell_type":"code","source":"print_test(test_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_fig(test_loader, 'test_plot.png')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}